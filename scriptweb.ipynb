{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e459005e",
   "metadata": {},
   "source": [
    "# ** AI & Cognition Project**\n",
    "# Web Engine\n",
    " \n",
    " 5 DS 1 - \n",
    " Group 1:\n",
    "\n",
    "*   Med Anas FATTOUM\n",
    "*   Iskander REGAIEG\n",
    "*   Youssef Aziz ZGHAL\n",
    "*   Haroun ELLEUCH\n",
    "*   Saida MAJBOUR\n",
    "*   Nadhir BOUHAOUALA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21886761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdfpandas.graph import to_dataframe\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "from utils2 import *\n",
    "from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, OWL, RDF, RDFS, SKOS, VOID, XMLNS, XSD\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph, ConjunctiveGraph\n",
    "from rdflib.extras import describer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af84d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bouzm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70bb0863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N936a658154b443698f473a81a5c67537 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse('ontology.owl', format='nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141e02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_dataframe(g)\n",
    "docs = df[ df['rdfs:comment{Literal}'].notna() ].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1920bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouzm\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1255, 49)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "data_input = docs['rdfs:comment{Literal}']\n",
    "\n",
    "for i in range(len(docs[\"rdf:type{URIRef}[0]\"])):\n",
    "    for title in str(docs[\"rdf:type{URIRef}[0]\"][i]).split(\"#\")[1:]:\n",
    "        title = title.replace('_',' ')\n",
    "        data_input[i] += \" \"+title\n",
    "    for title in str(docs[\"index\"][i]).split(\"#\")[1:]:\n",
    "        title = title.replace('_',' ')\n",
    "        data_input[i] += \" \"+title\n",
    "\n",
    "X = vectorizer.fit_transform(data_input)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df_vocab = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18039a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_parts(q, df):\n",
    "    q = [q]\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}\n",
    "    for i in range(df.shape[1]):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "\n",
    "    # Sort the values \n",
    "    sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Print the articles and their similarity values\n",
    "    result = []\n",
    "    for k, v in sim_sorted:\n",
    "         if v >= 0.085 :\n",
    "            title = ''\n",
    "            sub_title = ''\n",
    "            sub_sub_title = ''\n",
    "            parag = docs.iloc[k,:]['rdfs:comment{Literal}']\n",
    "            uri = str(docs.iloc[k,:][\"rdf:type{URIRef}[0]\"]).split(\"#\")\n",
    "            \n",
    "            if (len(uri) > 0):\n",
    "                if(uri[0]!='nan'):\n",
    "                    title = str(docs.iloc[k,:][\"rdf:type{URIRef}[0]\"]).split(\"#\")[1]\n",
    "                    sub_sub_title = \"\"\n",
    "                    if len(str(docs.iloc[k,:][\"rdf:type{URIRef}[0]\"]).split(\"#\")) > 1:\n",
    "                        sub_sub_title = str(docs.iloc[k,:][\"rdf:type{URIRef}[0]\"]).split(\"#\")[2]\n",
    "            if len(str(docs.iloc[k,:][\"index\"]).split(\"#\")) >= 1:\n",
    "                sub_title = docs.iloc[k,:][\"index\"].split(\"#\")[1] \n",
    "            result.append([title,sub_sub_title,sub_title,parag , v])\n",
    "            #print()\n",
    "    return result\n",
    "        #title , sub_sub title = individuals , sub title , annotation , probailit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60759a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [16/Nov/2021 20:28:59] \"GET /annotate?recherche=tool HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:29:02] \"GET /annotate?recherche=pmi HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:29:14] \"GET /annotate?recherche=pmi%20has%20input HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmi has input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:29:24] \"GET /annotate?recherche=project%20has%20risks HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project has risks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:29:39] \"GET /annotate?recherche=how%20many%20inputs%20does%20project%20risk%20management%20require HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many inputs does project risk management require\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:31:22] \"GET /annotate?recherche=ouputs HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:31:26] \"GET /annotate?recherche=outputs HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:31:48] \"GET /annotate?recherche=input HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:32:44] \"GET /annotate?recherche=How%20many%20inputs%20does%20project%20management%20require HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many inputs does project management require\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:38:04] \"GET /annotate?recherche=How%20many%20inputs%20does%20project%20risk%20management%20have%20%3F HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many inputs does project risk management have\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Nov/2021 20:38:38] \"GET /annotate?recherche= HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask_cors import CORS\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/annotate\",methods=['GET'])\n",
    "def index():\n",
    "    argum=(list(request.args.values()))\n",
    "    if(len(argum)!=1):\n",
    "        return \"passez un argument texte\"\n",
    "    l=((get_similar_parts(remove_stopwords(clean(argum[0])), df_vocab)))\n",
    "    print(remove_stopwords(clean(argum[0])))\n",
    "    if(l==None):\n",
    "        return \" ICI INDIV \"\n",
    "    if(len(l)==0):\n",
    "        return \" ICI INDIV \"\n",
    "    l=l[:13]\n",
    "    liste_annot=[clean_annot(j[3]) for j in l]\n",
    "    liste_indiv=[k[0].upper().replace('_',' ')+': '+k[1].replace('_',' ')+': '+k[2].replace('_',' ')+'?'+str(k[4]*100)[:4] for k in l ]\n",
    "    annotation=\"##\".join(liste_annot)\n",
    "    indiv=\"##\".join(liste_indiv)\n",
    "    return(annotation+\" ICI INDIV \"+indiv)\n",
    "    \n",
    "\n",
    "app.run(host='localhost',port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
